{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier Summary\n",
    "\n",
    "- For example, classify Invasive Carcinoma given a qa_answer.\n",
    "- p(Present | qa_answer) = p(Present)* p(word_1 | Present) * p(word_2 | Present)...\n",
    "- Take the max of p(Present | qa_answer) and p(Absent | qa_answer) and classify it.\n",
    "- prior probability: p(Present) = # of Present rows in Invasive Carcinoma / # of total rows in Invasive Carcinoma\n",
    "- likelihoods: p(word_1 | Present) = # of rows with word_1 in Present and Invasive Carcinoma / # of total rows in Present and Invasive Carcinoma\n",
    "- OR if word_1 doesn't exist in training data, p(word_1 | Present) = 1/1000000\n",
    "- prior and liklihoods are based off the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/clean/non_synoptic/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trevor.kwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "def calculate_likelihood(train_df, desired_label_keys):\n",
    "    \"\"\"\n",
    "    For each label_key and label_value combination pair, get the likelihood prob of all words in it. e.g. p(word | Present)\n",
    "    \"\"\"\n",
    "    # Initialize the results list (to be converted to a DataFrame)\n",
    "    likelihoods = []\n",
    "\n",
    "    # For a given label_key\n",
    "    for label_key in desired_label_keys:\n",
    "        # Filter the DataFrame by the current label_key\n",
    "        filtered_df = train_df[train_df['label_key'] == label_key]\n",
    "\n",
    "        # Get the unique label values for this label key\n",
    "        unique_label_values = filtered_df['label_value'].unique()\n",
    "\n",
    "        # For a given label_key and label_value pair...\n",
    "        for label_value in unique_label_values:\n",
    "            # Get the label_key and label_value subset df\n",
    "            value_filtered_df = filtered_df[filtered_df['label_value'] == label_value]\n",
    "\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokenized_sentences = value_filtered_df['answer'].apply(lambda x: [word.lower() for word in tokenizer.tokenize(x)])\n",
    "            # Breaks each answer in the subset df into words e.g. [negative, identifed] [positive, yes]...\n",
    "            # tokenized_sentences = value_filtered_df['answer'].apply(lambda x: [word.lower() for word in nltk.word_tokenize(x)])\n",
    "\n",
    "            # Gets the counts of each word in terms of how many times a word shows up in an answer and stores it in \"count\"\n",
    "            word_counts = Counter(word for words in tokenized_sentences for word in set(words))\n",
    "\n",
    "            # Get the number of rows in the label_key and label_value combination pair subset dataframe\n",
    "            total_rows = len(value_filtered_df)\n",
    "\n",
    "            # Calculate the probability of each word and add it to the likelihoods list\n",
    "            for word, count in word_counts.items():\n",
    "                likelihoods.append({\n",
    "                    'label_key': label_key,\n",
    "                    'label_value': label_value,\n",
    "                    'word': word,\n",
    "                    'prob': count / total_rows\n",
    "                })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    return pd.DataFrame(likelihoods)\n",
    "\n",
    "desired_label_keys = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "\n",
    "likelihood_df = calculate_likelihood(train_df, desired_label_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>word</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.746575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>margins</td>\n",
       "      <td>0.595890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>carcinoma</td>\n",
       "      <td>0.184932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>all</td>\n",
       "      <td>0.246575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>of</td>\n",
       "      <td>0.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>invasive</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>is</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>tumour</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>a</td>\n",
       "      <td>0.028571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>698 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           label_key label_value       word      prob\n",
       "0       DCIS Margins    Negative   negative  0.746575\n",
       "1       DCIS Margins    Negative    margins  0.595890\n",
       "2       DCIS Margins    Negative  carcinoma  0.184932\n",
       "3       DCIS Margins    Negative        all  0.246575\n",
       "4       DCIS Margins    Negative         of  0.178082\n",
       "..               ...         ...        ...       ...\n",
       "693  Tumour Focality      Single   invasive  0.057143\n",
       "694  Tumour Focality      Single          1  0.028571\n",
       "695  Tumour Focality      Single         is  0.028571\n",
       "696  Tumour Focality      Single     tumour  0.028571\n",
       "697  Tumour Focality      Single          a  0.028571\n",
       "\n",
       "[698 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Can't Be Assessed</td>\n",
       "      <td>0.006250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DCIS Margins</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.081250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ER Status</td>\n",
       "      <td>Can't Be Assessed</td>\n",
       "      <td>0.014706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ER Status</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.191176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ER Status</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extranodal Extension</td>\n",
       "      <td>Absent</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Extranodal Extension</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HER2 Status</td>\n",
       "      <td>equivocal</td>\n",
       "      <td>0.081967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HER2 Status</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.721311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HER2 Status</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.196721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Absent</td>\n",
       "      <td>0.146341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Invasive Carcinoma</td>\n",
       "      <td>Absent</td>\n",
       "      <td>0.179245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Invasive Carcinoma</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.820755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Invasive Carcinoma Margins</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.851190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Invasive Carcinoma Margins</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.148810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lymphovascular Invasion</td>\n",
       "      <td>Absent</td>\n",
       "      <td>0.757576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Lymphovascular Invasion</td>\n",
       "      <td>Cannot be determined</td>\n",
       "      <td>0.006061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lymphovascular Invasion</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.236364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Necrosis</td>\n",
       "      <td>Absent</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Necrosis</td>\n",
       "      <td>Present</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PR Status</td>\n",
       "      <td>Can't Be Assessed</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PR Status</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.293103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PR Status</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Multiple</td>\n",
       "      <td>0.477612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Tumour Focality</td>\n",
       "      <td>Single</td>\n",
       "      <td>0.522388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     label_key           label_value      prob\n",
       "0                 DCIS Margins     Can't Be Assessed  0.006250\n",
       "1                 DCIS Margins              Negative  0.912500\n",
       "2                 DCIS Margins              Positive  0.081250\n",
       "3                    ER Status     Can't Be Assessed  0.014706\n",
       "4                    ER Status              negative  0.191176\n",
       "5                    ER Status              positive  0.794118\n",
       "6         Extranodal Extension                Absent  0.478261\n",
       "7         Extranodal Extension               Present  0.521739\n",
       "8                  HER2 Status             equivocal  0.081967\n",
       "9                  HER2 Status              negative  0.721311\n",
       "10                 HER2 Status              positive  0.196721\n",
       "11            Insitu Component                Absent  0.146341\n",
       "12            Insitu Component               Present  0.853659\n",
       "13          Invasive Carcinoma                Absent  0.179245\n",
       "14          Invasive Carcinoma               Present  0.820755\n",
       "15  Invasive Carcinoma Margins              Negative  0.851190\n",
       "16  Invasive Carcinoma Margins              Positive  0.148810\n",
       "17     Lymphovascular Invasion                Absent  0.757576\n",
       "18     Lymphovascular Invasion  Cannot be determined  0.006061\n",
       "19     Lymphovascular Invasion               Present  0.236364\n",
       "20                    Necrosis                Absent  0.352941\n",
       "21                    Necrosis               Present  0.647059\n",
       "22                   PR Status     Can't Be Assessed  0.017241\n",
       "23                   PR Status              negative  0.293103\n",
       "24                   PR Status              positive  0.689655\n",
       "25             Tumour Focality              Multiple  0.477612\n",
       "26             Tumour Focality                Single  0.522388"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Counting the occurrences for each combination of label_key and label_value\n",
    "occurrences = train_df[train_df['label_key'].isin(desired_label_keys)].groupby(['label_key', 'label_value']).size().reset_index(name='count')\n",
    "\n",
    "# Counting the total occurrences for each label_key\n",
    "total_occurrences = occurrences.groupby('label_key')['count'].transform('sum')\n",
    "\n",
    "# Calculating the probability\n",
    "occurrences['prob'] = occurrences['count'] / total_occurrences\n",
    "\n",
    "# Creating the prior_prob DataFrame with the desired columns\n",
    "prior_prob = occurrences[['label_key', 'label_value', 'prob']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "prior_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "original_model_checkpoint = 'franklu/pubmed_bert_squadv2' # for qa\n",
    "original_model_name = original_model_checkpoint.split(\"/\")[-1]\n",
    "original_model_dir = \"../results/trained\\\\\"\n",
    "original_model_signature = '_19827_v2\\\\' # for qa\n",
    "version = \"v2\"\n",
    "\n",
    "foiset = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "# foiset = [\"Invasive Carcinoma\"]\n",
    "\n",
    "def classify(qa_answer, label_key, likelihood_df, prior_prob, debug_file, train_df):\n",
    "    if pd.isna(qa_answer) or str(qa_answer).strip() == '':\n",
    "        return 'No Mention', {}\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(str(qa_answer))]\n",
    "    # words = [word.lower() for word in nltk.word_tokenize(str(qa_answer))]\n",
    "    debug_file.write(f\"Words in the qa_answer row: {words}\\n\")\n",
    "\n",
    "    product_likelihoods = {}\n",
    "\n",
    "    label_values = prior_prob[prior_prob['label_key'] == label_key]['label_value'].unique()\n",
    "    for label_value in label_values:\n",
    "        prior = prior_prob[(prior_prob['label_key'] == label_key) & (prior_prob['label_value'] == label_value)]['prob'].iloc[0]\n",
    "        debug_file.write(f\"The prior probs of this row: {prior}\\n\")\n",
    "\n",
    "        subset_count = train_df[(train_df['label_key'] == label_key) & (train_df['label_value'] == label_value)].shape[0]\n",
    "        debug_file.write(f\"The subset count is: {subset_count}\\n\")\n",
    "\n",
    "        for word in words:\n",
    "            entry = likelihood_df[(likelihood_df['label_key'] == label_key) & (likelihood_df['label_value'] == label_value) & (likelihood_df['word'].str.lower() == word)]\n",
    "            debug_file.write(f\"The entry is:{entry}\\n\")\n",
    "\n",
    "            # If the entry is empty, set the likelihood as 1 divided by the number of rows in the subset\n",
    "            if entry.empty:\n",
    "                likelihood = 1/1000000\n",
    "                debug_file.write(f\"likelihood = 1/1,000,000\\n\")\n",
    "                # likelihood = 1 / (subset_count + 1)\n",
    "                # debug_file.write(f\"likelihood = 1/(subset_count+1)\\n\")\n",
    "            else:\n",
    "                likelihood = entry['prob'].iloc[0]\n",
    "                debug_file.write(f\"likelihood = prob\\n\")\n",
    "\n",
    "            if label_value not in product_likelihoods:\n",
    "                product_likelihoods[label_value] = prior * likelihood\n",
    "            else:\n",
    "                product_likelihoods[label_value] *= likelihood\n",
    "\n",
    "            debug_file.write(f\"Midpoint product likelihood: {product_likelihoods}\\n\")\n",
    "\n",
    "    debug_file.write(f\"Final product likelihoods: {product_likelihoods}\\n\")\n",
    "\n",
    "    return (max(product_likelihoods, key=product_likelihoods.get) if product_likelihoods else 'No match', product_likelihoods)\n",
    "\n",
    "\n",
    "# Initialize a dictionary to hold the overall accuracies for each \"foi\"\n",
    "overall_accuracies = {}\n",
    "\n",
    "# Initialize a dictionary to hold the accuracies for each label_value for each \"foi\"\n",
    "label_value_accuracies = {}\n",
    "\n",
    "# Loop over each \"foi\" (label_key) in foiset\n",
    "for foi in foiset:\n",
    "    # create the bayes directory if it doesn't exist\n",
    "    base_directory = original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\"\n",
    "    bayes_classification_directory = base_directory + foi + \"\\\\\" + \"bayes_classification\\\\\"\n",
    "    if not os.path.exists(bayes_classification_directory):\n",
    "        os.makedirs(bayes_classification_directory)\n",
    "    # create debug file\n",
    "    debug_filename = original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + \"bayes_classification\\\\\" + \"debug_output.txt\"\n",
    "    with open(debug_filename, 'w') as debug_file:\n",
    "        # Load the prediction CSV\n",
    "        pred_foi = pd.read_csv(original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + 'predictions_' + foi + '.csv')\n",
    "\n",
    "        # in val pred csvs, add No Mention\n",
    "        pred_foi['label_value'].fillna('No Mention', inplace=True)\n",
    "        pred_foi['label_value'].replace('', 'No Mention', inplace=True)\n",
    "\n",
    "        # Apply the classify function and store the results\n",
    "        # pred_foi['bayes_classification'] = pred_foi['qa_answer'].apply(lambda x: classify(x, foi, likelihood_df, prior_prob))\n",
    "\n",
    "        # Apply the classify function and store the results in two new columns\n",
    "        pred_foi['classification_results'] = pred_foi['qa_answer'].apply(lambda x: classify(x, foi, likelihood_df, prior_prob, debug_file, train_df))\n",
    "        pred_foi['bayes_classification'] = pred_foi['classification_results'].apply(lambda x: x[0])\n",
    "        pred_foi['prod_likelihoods'] = pred_foi['classification_results'].apply(lambda x: str(x[1]))  # Convert dictionary to string for saving\n",
    "\n",
    "        # Drop the temporary column used for storing both results\n",
    "        pred_foi.drop(columns=['classification_results'], inplace=True)\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        directory = original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + \"bayes_classification\\\\\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        # Save the new prediction CSV\n",
    "        pred_foi.to_csv(directory + 'pred_classified_' + foi + '.csv', index=False)\n",
    "\n",
    "        # Compute the overall accuracy for this \"foi\"\n",
    "        overall_accuracy = (pred_foi['bayes_classification'] == pred_foi['label_value']).mean()\n",
    "        overall_accuracies[foi] = overall_accuracy\n",
    "\n",
    "        # Compute the accuracy for each label_value for this \"foi\"\n",
    "        label_value_accuracy = pred_foi.groupby('label_value').apply(lambda df: (df['bayes_classification'] == df['label_value']).mean())\n",
    "        label_value_accuracies[foi] = label_value_accuracy\n",
    "\n",
    "        # Convert the label_value_accuracies to a DataFrame and save to a CSV file for this \"foi\"\n",
    "        label_value_accuracy_df = pd.DataFrame(label_value_accuracy, columns=['accuracy'])\n",
    "        label_value_accuracy_df.to_csv(directory + 'accuracies_' + foi + '.csv')\n",
    "\n",
    "# Convert the overall_accuracies to a DataFrame and save to a CSV file\n",
    "overall_accuracies_df = pd.DataFrame.from_dict(overall_accuracies, orient='index', columns=['accuracy'])\n",
    "\n",
    "# Convert the label_value_accuracies to a DataFrame and save to a CSV file\n",
    "label_value_accuracies_df = pd.concat({k: pd.Series(v) for k, v in label_value_accuracies.items()}).reset_index()\n",
    "label_value_accuracies_df.columns = ['foi', 'label_value', 'accuracy']\n",
    "\n",
    "# Append overall accuracies to label_value_accuracies_df\n",
    "overall_accuracies_df = overall_accuracies_df.reset_index().rename(columns={'index': 'foi', 'accuracy': 'accuracy'})\n",
    "combined_accuracies_df = pd.concat([label_value_accuracies_df, overall_accuracies_df], keys=['label_value_accuracy', 'overall_accuracy'], ignore_index=False)\n",
    "combined_accuracies_df.reset_index(level=0, inplace=True)\n",
    "combined_accuracies_df.rename(columns={'level_0': 'type'}, inplace=True)\n",
    "\n",
    "# Calculate the mean accuracy for \"No Mention\" across all FOIs\n",
    "no_mention_mean_accuracy = label_value_accuracies_df[label_value_accuracies_df['label_value'] == 'No Mention']['accuracy'].mean()\n",
    "# Create a DataFrame for the overall \"No Mention\" mean accuracy\n",
    "overall_no_mention_accuracy_df = pd.DataFrame({\n",
    "    'type': ['overall_accuracy'],\n",
    "    'foi': ['No Mention'],\n",
    "    'label_value': [None],\n",
    "    'accuracy': [no_mention_mean_accuracy]\n",
    "})\n",
    "# Concatenate the overall \"No Mention\" mean accuracy with the combined_accuracies_df\n",
    "combined_accuracies_df = pd.concat([combined_accuracies_df, overall_no_mention_accuracy_df], ignore_index=True)\n",
    "\n",
    "combined_accuracies_df.to_csv(original_model_dir + original_model_name + original_model_signature + 'eval\\\\' + version + \"\\\\\" + 'all_foi_bayes_accuracies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trevor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
