{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td># of Sentinel LN</td>\n",
       "      <td>1254</td>\n",
       "      <td>1255</td>\n",
       "      <td># of Sentinel LN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the Number of Sentinel Nodes Examined?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td># of Sentinel LN</td>\n",
       "      <td>1344</td>\n",
       "      <td>1345</td>\n",
       "      <td># of Sentinel LN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the Number of Sentinel Nodes Examined?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Estimated Size of Largest LN</td>\n",
       "      <td>3924</td>\n",
       "      <td>3931</td>\n",
       "      <td>Estimated Size of Largest LN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the Estimated Size of Largest Lymph no...</td>\n",
       "      <td>0.5 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Histologic Type (Insitu) - Other</td>\n",
       "      <td>914</td>\n",
       "      <td>946</td>\n",
       "      <td>Histologic Type (Insitu)</td>\n",
       "      <td>Other</td>\n",
       "      <td>What is the In Situ Component Type?</td>\n",
       "      <td>encapsulated papillary carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Estimated Size (IO)</td>\n",
       "      <td>957</td>\n",
       "      <td>962</td>\n",
       "      <td>Estimated Size</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the estimated size corresponding to an...</td>\n",
       "      <td>32 mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>565</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Architectural Patterns (ID) - cribriform</td>\n",
       "      <td>1986</td>\n",
       "      <td>1997</td>\n",
       "      <td>Architectural Patterns (ID)</td>\n",
       "      <td>cribriform</td>\n",
       "      <td>What are the architectural patterns correspond...</td>\n",
       "      <td>cribriform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>565</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Architectural Patterns (ID) - solid</td>\n",
       "      <td>2001</td>\n",
       "      <td>2007</td>\n",
       "      <td>Architectural Patterns (ID)</td>\n",
       "      <td>solid</td>\n",
       "      <td>What are the architectural patterns correspond...</td>\n",
       "      <td>solid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>565</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Necrosis (ID) - Present</td>\n",
       "      <td>2469</td>\n",
       "      <td>2499</td>\n",
       "      <td>Necrosis</td>\n",
       "      <td>Present</td>\n",
       "      <td>What is the necrosis?</td>\n",
       "      <td>with comedonecrosis identified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>565</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Comedo necrosis</td>\n",
       "      <td>2474</td>\n",
       "      <td>2489</td>\n",
       "      <td>Comedo Necrosis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the necrosis type?</td>\n",
       "      <td>comedonecrosis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>565</td>\n",
       "      <td>\\n                                       AGE/S...</td>\n",
       "      <td>Estimated Size of Largest LN</td>\n",
       "      <td>5710</td>\n",
       "      <td>5728</td>\n",
       "      <td>Estimated Size of Largest LN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What is the Estimated Size of Largest Lymph no...</td>\n",
       "      <td>1.6 x 0.4 x 0.3 cm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4208 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      report_id                                               text  \\\n",
       "0             2  \\n                                       AGE/S...   \n",
       "1             2  \\n                                       AGE/S...   \n",
       "2             2  \\n                                       AGE/S...   \n",
       "3             2  \\n                                       AGE/S...   \n",
       "4             2  \\n                                       AGE/S...   \n",
       "...         ...                                                ...   \n",
       "4203        565  \\n                                       AGE/S...   \n",
       "4204        565  \\n                                       AGE/S...   \n",
       "4205        565  \\n                                       AGE/S...   \n",
       "4206        565  \\n                                       AGE/S...   \n",
       "4207        565  \\n                                       AGE/S...   \n",
       "\n",
       "                                         label  start   end  \\\n",
       "0                             # of Sentinel LN   1254  1255   \n",
       "1                             # of Sentinel LN   1344  1345   \n",
       "2                 Estimated Size of Largest LN   3924  3931   \n",
       "3             Histologic Type (Insitu) - Other    914   946   \n",
       "4                          Estimated Size (IO)    957   962   \n",
       "...                                        ...    ...   ...   \n",
       "4203  Architectural Patterns (ID) - cribriform   1986  1997   \n",
       "4204       Architectural Patterns (ID) - solid   2001  2007   \n",
       "4205                   Necrosis (ID) - Present   2469  2499   \n",
       "4206                           Comedo necrosis   2474  2489   \n",
       "4207              Estimated Size of Largest LN   5710  5728   \n",
       "\n",
       "                         label_key label_value  \\\n",
       "0                 # of Sentinel LN         NaN   \n",
       "1                 # of Sentinel LN         NaN   \n",
       "2     Estimated Size of Largest LN         NaN   \n",
       "3         Histologic Type (Insitu)       Other   \n",
       "4                   Estimated Size         NaN   \n",
       "...                            ...         ...   \n",
       "4203   Architectural Patterns (ID)  cribriform   \n",
       "4204   Architectural Patterns (ID)       solid   \n",
       "4205                      Necrosis     Present   \n",
       "4206               Comedo Necrosis         NaN   \n",
       "4207  Estimated Size of Largest LN         NaN   \n",
       "\n",
       "                                               question  \\\n",
       "0        What is the Number of Sentinel Nodes Examined?   \n",
       "1        What is the Number of Sentinel Nodes Examined?   \n",
       "2     What is the Estimated Size of Largest Lymph no...   \n",
       "3                   What is the In Situ Component Type?   \n",
       "4     What is the estimated size corresponding to an...   \n",
       "...                                                 ...   \n",
       "4203  What are the architectural patterns correspond...   \n",
       "4204  What are the architectural patterns correspond...   \n",
       "4205                              What is the necrosis?   \n",
       "4206                         What is the necrosis type?   \n",
       "4207  What is the Estimated Size of Largest Lymph no...   \n",
       "\n",
       "                                answer  \n",
       "0                                    1  \n",
       "1                                    1  \n",
       "2                               0.5 cm  \n",
       "3     encapsulated papillary carcinoma  \n",
       "4                                32 mm  \n",
       "...                                ...  \n",
       "4203                       cribriform   \n",
       "4204                            solid   \n",
       "4205    with comedonecrosis identified  \n",
       "4206                   comedonecrosis   \n",
       "4207                1.6 x 0.4 x 0.3 cm  \n",
       "\n",
       "[4208 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/clean/non_synoptic/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trevor.kwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "# Function to calculate weights\n",
    "def calculate_weights(train_df, desired_label_keys):\n",
    "    # Initialize the results list (to be converted to a DataFrame)\n",
    "    weights = []\n",
    "\n",
    "    # Loop over the desired label keys\n",
    "    for label_key in desired_label_keys:\n",
    "        # Filter the DataFrame by the current label_key\n",
    "        filtered_df = train_df[train_df['label_key'] == label_key]\n",
    "\n",
    "        # Get the unique label values for this label key\n",
    "        unique_label_values = filtered_df['label_value'].unique()\n",
    "\n",
    "        # Loop over the unique label values\n",
    "        for label_value in unique_label_values:\n",
    "            # Further filter the DataFrame by the current label_value\n",
    "            value_filtered_df = filtered_df[filtered_df['label_value'] == label_value]\n",
    "\n",
    "            # Tokenize the 'answer' column into words, convert to lowercase, and flatten the list\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            words = [word.lower() for sentence in value_filtered_df['answer'] for word in tokenizer.tokenize(sentence)]\n",
    "            # words = [word.lower() for sentence in value_filtered_df['answer'] for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "            # Count the occurrences of each word\n",
    "            word_counts = Counter(words)\n",
    "\n",
    "            # Calculate the total number of words\n",
    "            total_words = sum(word_counts.values())\n",
    "\n",
    "            # Calculate the weight of each word and add it to the weights list\n",
    "            for word, count in word_counts.items():\n",
    "                weights.append({\n",
    "                    'label_key': label_key,\n",
    "                    'label_value': label_value,\n",
    "                    'word': word,\n",
    "                    'weight': count / total_words\n",
    "                })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    return pd.DataFrame(weights)\n",
    "\n",
    "desired_label_keys = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "\n",
    "weights_df = calculate_weights(train_df, desired_label_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Present</td>\n",
       "      <td>identified</td>\n",
       "      <td>0.003722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Absent</td>\n",
       "      <td>identified</td>\n",
       "      <td>0.099338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label_key label_value        word    weight\n",
       "264  Insitu Component     Present  identified  0.003722\n",
       "295  Insitu Component      Absent  identified  0.099338"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = weights_df[weights_df['label_key'] == 'Insitu Component']\n",
    "df[df['word'] == 'identified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label_key, label_value, word, weight]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df[weights_df['word'] == '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trevor.kwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifications_and_weights: ('No Mention', {})\n",
      "classifications_and_weights: ('negative', {'negative': 0.8666666666666667})\n",
      "classifications_and_weights: ('Absent', {'Present': 0.24050632911392406, 'Absent': 0.2631578947368421})\n",
      "classifications_and_weights: ('negative', {'negative': 0.8723404255319148})\n",
      "classifications_and_weights: ('Absent', {'Absent': 0.15894039735099336, 'Present': 0.0037220843672456576})\n",
      "classifications_and_weights: ('Absent', {'Present': 0.5384615384615384, 'Absent': 0.5882352941176471})\n",
      "classifications_and_weights: ('Negative', {'Negative': 0.2686945500633714, 'Positive': 0.08227848101265824})\n",
      "classifications_and_weights: ('Absent', {'Present': 0.3722627737226277, 'Absent': 0.3772727272727273, 'Cannot be determined': 0.3333333333333333})\n",
      "classifications_and_weights: ('No Mention', {})\n",
      "classifications_and_weights: ('negative', {'negative': 0.6296296296296297})\n",
      "classifications_and_weights: ('No Mention', {})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "original_model_checkpoint = 'franklu/pubmed_bert_squadv2' # for qa\n",
    "original_model_name = original_model_checkpoint.split(\"/\")[-1]\n",
    "original_model_dir = \"../results/trained\\\\\"\n",
    "original_model_signature = '_19827_v2\\\\' # for qa\n",
    "version = \"v2\"\n",
    "\n",
    "# Assuming foiset is your list of \"foi\"s\n",
    "# foiset = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "foiset = ['Insitu Component', 'Invasive Carcinoma']\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "def classify(row, weights_df):\n",
    "    if pd.isna(row[\"qa_answer\"]) or str(row[\"qa_answer\"]).strip() == '':\n",
    "        return 'No Mention', {}\n",
    "    \n",
    "    # Tokenize the 'qa_answer' column into words, convert to lowercase\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(row['qa_answer'])]\n",
    "    # words = [word.lower() for word in nltk.word_tokenize(row['qa_answer'])]\n",
    "    \n",
    "    # Initialize the dictionary for storing the sum of weights for each label_value\n",
    "    sum_weights = {}\n",
    "    \n",
    "    # Loop over each word in the answer\n",
    "    for word in words:\n",
    "        # Find all the corresponding entries in weights_df, ensure comparison is also case-insensitive\n",
    "        entries = weights_df[weights_df['word'].str.lower() == word]\n",
    "        \n",
    "        # If the word is not in weights_df, continue to the next word\n",
    "        if entries.empty:\n",
    "            continue\n",
    "        \n",
    "        # for each time the word appears in weights_df, add the weight to the corresponding label_value in sum_weights\n",
    "        for _, entry in entries.iterrows(): # for index, entry (loops over the rows of the entries df) entries df is all the rows in weights_df that include the \"word\" being searched (entry is one row)\n",
    "            if entry['label_value'] not in sum_weights: # init the first label_value and weight to sum_weights\n",
    "                sum_weights[entry['label_value']] = entry['weight']\n",
    "            else:\n",
    "                sum_weights[entry['label_value']] += entry['weight']\n",
    "    \n",
    "    # If no weights were found, return 'No match'\n",
    "    if not sum_weights:\n",
    "        return 'No match', sum_weights\n",
    "    \n",
    "    # Return the label_value with the highest sum of weights\n",
    "    return max(sum_weights, key=sum_weights.get), sum_weights\n",
    "\n",
    "# Initialize a dictionary to hold the overall accuracies for each \"foi\"\n",
    "overall_accuracies = {}\n",
    "\n",
    "# Initialize a dictionary to hold the accuracies for each label_value for each \"foi\"\n",
    "label_value_accuracies = {}\n",
    "\n",
    "for foi in foiset:\n",
    "    # Subset the weights_df for the current foi\n",
    "    weights_foi_df = weights_df[weights_df['label_key'] == foi]\n",
    "\n",
    "    pred_foi = pd.read_csv(original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + 'predictions_' + foi + '.csv')\n",
    "    pred_foi = pred_foi[['qa_answer', 'label_value']]\n",
    "\n",
    "    # in val pred csvs, add No Mention\n",
    "    pred_foi['label_value'].fillna('No Mention', inplace=True)\n",
    "    pred_foi['label_value'].replace('', 'No Mention', inplace=True)\n",
    "\n",
    "    # Create a DataFrame to hold the sum of weights\n",
    "    weights_sum_df = pd.DataFrame()\n",
    "\n",
    "    # Apply the modified classify function and store the results\n",
    "    classifications_and_weights = pred_foi.apply(lambda row: classify(row, weights_foi_df), axis=1)\n",
    "    zero_class = classifications_and_weights[0]\n",
    "    print(\"classifications_and_weights:\", zero_class)\n",
    "    pred_foi['weights_classification'] = [x[0] for x in classifications_and_weights]\n",
    "    weights_sum_df = pd.concat([weights_sum_df, pd.DataFrame.from_records([x[1] for x in classifications_and_weights])])\n",
    "\n",
    "    # Compute the overall accuracy for this \"foi\"\n",
    "    overall_accuracy = (pred_foi['weights_classification'] == pred_foi['label_value']).mean()\n",
    "    overall_accuracies[foi] = overall_accuracy\n",
    "\n",
    "    # Compute the accuracy for each label_value for this \"foi\"\n",
    "    label_value_accuracy = pred_foi.groupby('label_value').apply(lambda df: (df['weights_classification'] == df['label_value']).mean())\n",
    "    label_value_accuracies[foi] = label_value_accuracy\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + \"weights_classification\\\\\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the predictions for this \"foi\" to a CSV file\n",
    "    pred_foi.to_csv(directory + 'pred_classified_' + foi + '.csv', index=False)\n",
    "\n",
    "    # Save the sum of weights to a CSV file\n",
    "    weights_sum_df.to_csv(directory + 'weights_sum_' + foi + '.csv', index=False)\n",
    "\n",
    "    # Convert the label_value_accuracies to a DataFrame and save to a CSV file for this \"foi\"\n",
    "    label_value_accuracy_df = pd.DataFrame(label_value_accuracy, columns=['accuracy'])\n",
    "    label_value_accuracy_df.to_csv(directory + 'accuracies_' + foi + '.csv')\n",
    "\n",
    "# Convert the overall_accuracies to a DataFrame and save to a CSV file\n",
    "overall_accuracies_df = pd.DataFrame.from_dict(overall_accuracies, orient='index', columns=['accuracy'])\n",
    "\n",
    "# Convert the label_value_accuracies to a DataFrame and save to a CSV file\n",
    "label_value_accuracies_df = pd.concat({k: pd.Series(v) for k, v in label_value_accuracies.items()}).reset_index()\n",
    "label_value_accuracies_df.columns = ['foi', 'label_value', 'accuracy']\n",
    "\n",
    "# Append overall accuracies to label_value_accuracies_df\n",
    "overall_accuracies_df = overall_accuracies_df.reset_index().rename(columns={'index': 'foi', 'accuracy': 'accuracy'})\n",
    "combined_accuracies_df = pd.concat([label_value_accuracies_df, overall_accuracies_df], keys=['label_value_accuracy', 'overall_accuracy'], ignore_index=False)\n",
    "combined_accuracies_df.reset_index(level=0, inplace=True)\n",
    "combined_accuracies_df.rename(columns={'level_0': 'type'}, inplace=True)\n",
    "\n",
    "# Calculate the mean accuracy for \"No Mention\" across all FOIs\n",
    "no_mention_mean_accuracy = label_value_accuracies_df[label_value_accuracies_df['label_value'] == 'No Mention']['accuracy'].mean()\n",
    "# Create a DataFrame for the overall \"No Mention\" mean accuracy\n",
    "overall_no_mention_accuracy_df = pd.DataFrame({\n",
    "    'type': ['overall_accuracy'],\n",
    "    'foi': ['No Mention'],\n",
    "    'label_value': [None],\n",
    "    'accuracy': [no_mention_mean_accuracy]\n",
    "})\n",
    "# Concatenate the overall \"No Mention\" mean accuracy with the combined_accuracies_df\n",
    "combined_accuracies_df = pd.concat([combined_accuracies_df, overall_no_mention_accuracy_df], ignore_index=True)\n",
    "\n",
    "combined_accuracies_df.to_csv(original_model_dir + original_model_name + original_model_signature + 'eval\\\\' + version + \"\\\\\" + 'all_foi_weight_accuracies.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trevor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
