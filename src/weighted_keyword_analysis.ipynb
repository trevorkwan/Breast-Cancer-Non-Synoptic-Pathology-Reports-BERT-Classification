{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/clean/non_synoptic/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trevor.kwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "# Function to calculate weights\n",
    "def calculate_weights(train_df, desired_label_keys):\n",
    "    # Initialize the results list (to be converted to a DataFrame)\n",
    "    weights = []\n",
    "\n",
    "    # Loop over the desired label keys\n",
    "    for label_key in desired_label_keys:\n",
    "        # Filter the DataFrame by the current label_key\n",
    "        filtered_df = train_df[train_df['label_key'] == label_key]\n",
    "\n",
    "        # Get the unique label values for this label key\n",
    "        unique_label_values = filtered_df['label_value'].unique()\n",
    "\n",
    "        # Loop over the unique label values\n",
    "        for label_value in unique_label_values:\n",
    "            # Further filter the DataFrame by the current label_value\n",
    "            value_filtered_df = filtered_df[filtered_df['label_value'] == label_value]\n",
    "\n",
    "            # Tokenize the 'answer' column into words, convert to lowercase, and flatten the list\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            words = [word.lower() for sentence in value_filtered_df['answer'] for word in tokenizer.tokenize(sentence)]\n",
    "            # words = [word.lower() for sentence in value_filtered_df['answer'] for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "            # Count the occurrences of each word\n",
    "            word_counts = Counter(words)\n",
    "\n",
    "            # Calculate the total number of words\n",
    "            total_words = sum(word_counts.values())\n",
    "\n",
    "            # Calculate the weight of each word and add it to the weights list\n",
    "            for word, count in word_counts.items():\n",
    "                weights.append({\n",
    "                    'label_key': label_key,\n",
    "                    'label_value': label_value,\n",
    "                    'word': word,\n",
    "                    'weight': count / total_words\n",
    "                })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    return pd.DataFrame(weights)\n",
    "\n",
    "desired_label_keys = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "\n",
    "weights_df = calculate_weights(train_df, desired_label_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Present</td>\n",
       "      <td>identified</td>\n",
       "      <td>0.003722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Insitu Component</td>\n",
       "      <td>Absent</td>\n",
       "      <td>identified</td>\n",
       "      <td>0.099338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label_key label_value        word    weight\n",
       "264  Insitu Component     Present  identified  0.003722\n",
       "295  Insitu Component      Absent  identified  0.099338"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = weights_df[weights_df['label_key'] == 'Insitu Component']\n",
    "df[df['word'] == 'identified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_key</th>\n",
       "      <th>label_value</th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [label_key, label_value, word, weight]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_df[weights_df['word'] == '(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\trevor.kwan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifications_and_weights: ('Absent', {'Absent': 0.15894039735099336, 'Present': 0.0037220843672456576})\n",
      "classifications_and_weights: ('Absent', {'Present': 0.5384615384615384, 'Absent': 0.5882352941176471})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "original_model_checkpoint = 'franklu/pubmed_bert_squadv2' # for qa\n",
    "original_model_name = original_model_checkpoint.split(\"/\")[-1]\n",
    "original_model_dir = \"../results/trained\\\\\"\n",
    "original_model_signature = '_19827_v2\\\\' # for qa\n",
    "version = \"v2\"\n",
    "\n",
    "# Assuming foiset is your list of \"foi\"s\n",
    "# foiset = [\"DCIS Margins\", \"ER Status\", \"Extranodal Extension\", \"HER2 Status\", \"Insitu Component\", \"Invasive Carcinoma\", \"Invasive Carcinoma Margins\", \"Lymphovascular Invasion\", \"Necrosis\", \"PR Status\", \"Tumour Focality\"]\n",
    "foiset = ['Insitu Component', 'Invasive Carcinoma']\n",
    "\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer\n",
    "\n",
    "def classify(row, weights_df):\n",
    "    if pd.isna(row[\"qa_answer\"]) or str(row[\"qa_answer\"]).strip() == '':\n",
    "        return 'No Mention', {}\n",
    "    \n",
    "    # Tokenize the 'qa_answer' column into words, convert to lowercase\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = [word.lower() for word in tokenizer.tokenize(row['qa_answer'])]\n",
    "    # words = [word.lower() for word in nltk.word_tokenize(row['qa_answer'])]\n",
    "    \n",
    "    # Initialize the dictionary for storing the sum of weights for each label_value\n",
    "    sum_weights = {}\n",
    "    \n",
    "    # Loop over each word in the answer\n",
    "    for word in words:\n",
    "        # Find all the corresponding entries in weights_df, ensure comparison is also case-insensitive\n",
    "        entries = weights_df[weights_df['word'].str.lower() == word]\n",
    "        \n",
    "        # If the word is not in weights_df, continue to the next word\n",
    "        if entries.empty:\n",
    "            continue\n",
    "        \n",
    "        # for each time the word appears in weights_df, add the weight to the corresponding label_value in sum_weights\n",
    "        for _, entry in entries.iterrows(): # for index, entry (loops over the rows of the entries df) entries df is all the rows in weights_df that include the \"word\" being searched (entry is one row)\n",
    "            if entry['label_value'] not in sum_weights: # init the first label_value and weight to sum_weights\n",
    "                sum_weights[entry['label_value']] = entry['weight']\n",
    "            else:\n",
    "                sum_weights[entry['label_value']] += entry['weight']\n",
    "    \n",
    "    # If no weights were found, return 'No match'\n",
    "    if not sum_weights:\n",
    "        return 'No match', sum_weights\n",
    "    \n",
    "    # Return the label_value with the highest sum of weights\n",
    "    return max(sum_weights, key=sum_weights.get), sum_weights\n",
    "\n",
    "# Initialize a dictionary to hold the overall accuracies for each \"foi\"\n",
    "overall_accuracies = {}\n",
    "\n",
    "# Initialize a dictionary to hold the accuracies for each label_value for each \"foi\"\n",
    "label_value_accuracies = {}\n",
    "\n",
    "for foi in foiset:\n",
    "    # Subset the weights_df for the current foi\n",
    "    weights_foi_df = weights_df[weights_df['label_key'] == foi]\n",
    "\n",
    "    pred_foi = pd.read_csv(original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + 'predictions_' + foi + '.csv')\n",
    "    pred_foi = pred_foi[['qa_answer', 'label_value']]\n",
    "\n",
    "    # in val pred csvs, add No Mention\n",
    "    pred_foi['label_value'].fillna('No Mention', inplace=True)\n",
    "    pred_foi['label_value'].replace('', 'No Mention', inplace=True)\n",
    "\n",
    "    # Create a DataFrame to hold the sum of weights\n",
    "    weights_sum_df = pd.DataFrame()\n",
    "\n",
    "    # Apply the modified classify function and store the results\n",
    "    classifications_and_weights = pred_foi.apply(lambda row: classify(row, weights_foi_df), axis=1)\n",
    "    zero_class = classifications_and_weights[0]\n",
    "    print(\"classifications_and_weights:\", zero_class)\n",
    "    pred_foi['weights_classification'] = [x[0] for x in classifications_and_weights]\n",
    "    weights_sum_df = pd.concat([weights_sum_df, pd.DataFrame.from_records([x[1] for x in classifications_and_weights])])\n",
    "\n",
    "    # Compute the overall accuracy for this \"foi\"\n",
    "    overall_accuracy = (pred_foi['weights_classification'] == pred_foi['label_value']).mean()\n",
    "    overall_accuracies[foi] = overall_accuracy\n",
    "\n",
    "    # Compute the accuracy for each label_value for this \"foi\"\n",
    "    label_value_accuracy = pred_foi.groupby('label_value').apply(lambda df: (df['weights_classification'] == df['label_value']).mean())\n",
    "    label_value_accuracies[foi] = label_value_accuracy\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    directory = original_model_dir + original_model_name + original_model_signature + \"eval\\\\\" + version + \"\\\\\" + foi + \"\\\\\" + \"weights_classification\\\\\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the predictions for this \"foi\" to a CSV file\n",
    "    pred_foi.to_csv(directory + 'pred_classified_' + foi + '.csv', index=False)\n",
    "\n",
    "    # Save the sum of weights to a CSV file\n",
    "    weights_sum_df.to_csv(directory + 'weights_sum_' + foi + '.csv', index=False)\n",
    "\n",
    "    # Convert the label_value_accuracies to a DataFrame and save to a CSV file for this \"foi\"\n",
    "    label_value_accuracy_df = pd.DataFrame(label_value_accuracy, columns=['accuracy'])\n",
    "    label_value_accuracy_df.to_csv(directory + 'accuracies_' + foi + '.csv')\n",
    "\n",
    "# Convert the overall_accuracies to a DataFrame and save to a CSV file\n",
    "overall_accuracies_df = pd.DataFrame.from_dict(overall_accuracies, orient='index', columns=['accuracy'])\n",
    "\n",
    "# Convert the label_value_accuracies to a DataFrame and save to a CSV file\n",
    "label_value_accuracies_df = pd.concat({k: pd.Series(v) for k, v in label_value_accuracies.items()}).reset_index()\n",
    "label_value_accuracies_df.columns = ['foi', 'label_value', 'accuracy']\n",
    "\n",
    "# Append overall accuracies to label_value_accuracies_df\n",
    "overall_accuracies_df = overall_accuracies_df.reset_index().rename(columns={'index': 'foi', 'accuracy': 'accuracy'})\n",
    "combined_accuracies_df = pd.concat([label_value_accuracies_df, overall_accuracies_df], keys=['label_value_accuracy', 'overall_accuracy'], ignore_index=False)\n",
    "combined_accuracies_df.reset_index(level=0, inplace=True)\n",
    "combined_accuracies_df.rename(columns={'level_0': 'type'}, inplace=True)\n",
    "\n",
    "# Calculate the mean accuracy for \"No Mention\" across all FOIs\n",
    "no_mention_mean_accuracy = label_value_accuracies_df[label_value_accuracies_df['label_value'] == 'No Mention']['accuracy'].mean()\n",
    "# Create a DataFrame for the overall \"No Mention\" mean accuracy\n",
    "overall_no_mention_accuracy_df = pd.DataFrame({\n",
    "    'type': ['overall_accuracy'],\n",
    "    'foi': ['No Mention'],\n",
    "    'label_value': [None],\n",
    "    'accuracy': [no_mention_mean_accuracy]\n",
    "})\n",
    "# Concatenate the overall \"No Mention\" mean accuracy with the combined_accuracies_df\n",
    "combined_accuracies_df = pd.concat([combined_accuracies_df, overall_no_mention_accuracy_df], ignore_index=True)\n",
    "\n",
    "combined_accuracies_df.to_csv(original_model_dir + original_model_name + original_model_signature + 'eval\\\\' + version + \"\\\\\" + 'all_foi_weight_accuracies.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trevor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
